# Queue + Worker (BullMQ + Redis) â€” Learning Notes ğŸ“

> Project: **Search Hub** Â· Focus: make **indexing async** so the API stays fast, reliable, and scalable.

---

## Rules of thumb
- If itâ€™s user-facing but heavy â†’ queue it.

- Truth belongs in a database.

- Design for retries (make steps idempotent; throw on failure).

- Name queues with constants; validate payloads at both ends.

- One place to log completion; noisy logs hide real issues.

## 0) What I actually built (in my own words)

- When I `POST /v1/documents`, the API:
  1. Creates a **Document** row in Postgres (this is the real business record).
  2. Writes an **IndexJob** row with `status = queued` (truth + audit trail).
  3. **Enqueues** a BullMQ job to Redis with `{ tenantId, documentId }` and retries/backoff.
  4. Returns **202 Accepted** immediately.

- A separate **Worker** process subscribes to the same queue name and:
  1. Moves DB state: `queued â†’ processing`.
  2. Does the work (today a simulated sleep; tomorrow: extract â†’ embed â†’ upsert).
  3. Moves DB state: `processing â†’ indexed` (or `failed` with an error message).
  4. Lets BullMQ handle retries/backoff automatically if my code throws.

---

## 1) Why async? (what clicked for me)

- **Latency:** indexing is not user-facing; donâ€™t hold the request open.
- **Reliability:** background jobs can retry safely and survive crashes.
- **Scale:** workers are cheap to scale horizontally; the API doesnâ€™t care.

**Mental model Iâ€™ll remember:**
- Postgres = **source of truth** (what should/has happened)
- Redis = **delivery rail** (move work around + retry engine)
- Worker = **execution unit** (my code, not a magic Redis thing)

---

## 2) â€œAha!â€ moments

- The **worker is not created by Redis**. Itâ€™s created by *my Node process* when `new Worker(...)` runs.
- **Order matters**: *Document â†’ IndexJob â†’ enqueue*. If enqueue fails, I still have a durable `queued` row to reconcile later.
- **At-least-once delivery** is the norm. Design idempotent transitions (filtered `updateMany`) and make the processing step safe to retry.
- Use a **constant queue name** (e.g., `JOBS.INDEX_DOCUMENT`) so API + Worker never drift.

---

## 3) Small design decisions Iâ€™m proud of

- **Filtered state transitions** (idempotent):
  - `updateMany where status='queued'` â†’ `processing`
  - `updateMany where status='processing'` â†’ `indexed|failed`
- **Compound index** for worker speed: `@@index([tenantId, documentId, status])`.
- Optional **de-dup**: `@@unique([tenantId, documentId, status])` (treat unique violation as â€œalready queuedâ€).
- **Retries with exponential backoff** in BullMQ (`attempts: 3`, `1s, 2s, 4s`).
- **RemoveOnComplete** in Redis; keep truth in Postgres.

---

## 4) How the pieces map (tiny ASCII diagram)
```
Client
â”‚
â”‚ POST /v1/documents (202)
â–¼
API (apps/api)
â”œâ”€ Postgres: Document (insert)
â”œâ”€ Postgres: IndexJob { queued }
â””â”€ Redis: add job to queue "index:document"
â”‚
â–¼
Worker (apps/worker)
â”œâ”€ Postgres: queued â†’ processing
â”œâ”€ Do work (extract/embed/upsertâ€¦)
â””â”€ Postgres: processing â†’ indexed | failed
```

---

## 5) Trade-offs 

- DB row before enqueue (chosen): durable intent, easy recovery.
â†” Enqueue first: risk of â€œghost jobsâ€ if crash before DB write.

- Keep truth in DB (chosen): operational insight + reconciliation.
â†” Redis-only: faster, but no status/history â†’ hard to debug/communicate.

- Unique constraint (optional): prevents duplicate active jobs, but requires handling unique violations (or using upsert).

- QueueEvents vs worker events: one source of â€œcompleted/failedâ€ logs is enough; two = noisy.

## 6) Pitfalls I hit (and how I fixed them)

- Hot reload started the worker twice
Symptom: multiple â€œWorker startedâ€ lines.
Fix: narrow watch paths (tsx --ignore ...), ensure Turbo isnâ€™t launching it twice, and use global guards for QueueEvents.

- Double completion logs
Symptom: â€œcompletedâ€ printed twice.
Fix: log from either QueueEvents or worker.on('completed'), not both.

- Worker logs labeled as â€œapiâ€
Fix: logger.child({ service: 'worker' }) so streams are distinguishable.

- Job stuck in queued (Redis glitch)
Fix: DB still shows queued; I can re-enqueue from a small script that scans IndexJob where status='queued'.
